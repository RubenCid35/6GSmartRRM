{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHW4PfK_tKs4",
        "outputId": "b999ceab-da3f-42f5-bab6-6fface7b15b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '6GSmartRRM' already exists and is not an empty directory.\n",
            "mv: cannot stat '6GSmartRRM/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "COLAB: bool = True\n",
        "if COLAB:\n",
        "  !git clone https://github.com/RubenCid35/6GSmartRRM\n",
        "  !mv 6GSmartRRM/* .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMaOfDeTqDxT",
        "outputId": "0156a5bd-04df-4ab6-a480-40a64cb81e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt25cu124)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt25cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt25cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt25cu124)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiUAh6llKD_n"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2mAPEEgs4Bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f173722a-d6f5-4f7f-e01f-5f730b6c2a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrubencid001\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Simulation Parameters: \n",
              "\n",
              "|                      name |                     value |\n",
              "---------------------------------------------------------\n",
              "|        num_of_subnetworks |                   20.0000 |\n",
              "|              n_subchannel |                    4.0000 |\n",
              "|             deploy_length |                   20.0000 |\n",
              "|             subnet_radius |                    1.0000 |\n",
              "|                      minD |                    0.8000 |\n",
              "|               minDistance |                    2.0000 |\n",
              "|                 bandwidth |             40000000.0000 |\n",
              "|              ch_bandwidth |             10000000.0000 |\n",
              "|                        fc |           6000000000.0000 |\n",
              "|                    lambdA |                    0.0500 |\n",
              "|                  clutType |                     dense |\n",
              "|                  clutSize |                    2.0000 |\n",
              "|                  clutDens |                    0.6000 |\n",
              "|                   shadStd |                    7.2000 |\n",
              "|                 max_power |                    0.0000 |\n",
              "|                    no_dbm |                 -174.0000 |\n",
              "|           noise_figure_db |                    5.0000 |\n",
              "|               noise_power |                    0.0000 |\n",
              "|       correlationDistance |                    5.0000 |\n",
              "|            transmit_power |                    0.0010 |\n",
              "---------------------------------------------------------"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# simple data manipulation\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "# deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lrs\n",
        "from   torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv, GatedGraphConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# results logging\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# progress bar\n",
        "from   tqdm.notebook import tqdm, trange\n",
        "\n",
        "# remove warnings (remove deprecated warnings)\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "# visualization of resultsa\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from   matplotlib.ticker import MaxNLocator\n",
        "import seaborn           as sns\n",
        "\n",
        "# Graph Algorithms.\n",
        "import networkx as nx\n",
        "\n",
        "# Google Colab (many lines are removed)\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "# wheter we are using colab or not\n",
        "if not COLAB and not os.path.exists('./data/simulations'):\n",
        "    os.chdir('..')\n",
        "\n",
        "# Simulation Settings\n",
        "from g6smart.sim_config import SimConfig\n",
        "from g6smart.evaluation import rate_torch as rate_metrics\n",
        "\n",
        "config = SimConfig(0)\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4RFox2hs4Be"
      },
      "outputs": [],
      "source": [
        "def setup_wandb(name: str, group: str, config: dict[str, float], id: str = None):\n",
        "    config['name'] = name\n",
        "    return wandb.init(\n",
        "        project=\"6GSmartRRM\",\n",
        "        name   = name,\n",
        "        id     = id,\n",
        "        group  = group,\n",
        "        config = config,\n",
        "        resume = \"allow\" if id is None else \"must\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTRBs_8Is4Bf"
      },
      "source": [
        "## Simulations and Information\n",
        "\n",
        "Thanks to the given scripts, we can load a group of generated simulations. They don't have any solutions (neither approximations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAHxSAoFs4Bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e646b19a-fd2e-42ab-863d-e5d97c9e6e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Name of the already simulated data: \n",
            "\n",
            " ----> simulations-200K-sisa.zip\n"
          ]
        }
      ],
      "source": [
        "# Moung Google Drive Code\n",
        "if COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Move Simulations to avoid cluttering the drive folder\n",
        "    if not os.path.exists('/content/simulations'):\n",
        "      os.mkdir('/content/simulations')\n",
        "\n",
        "    if list(os.listdir('/content/simulations')) == []:\n",
        "      copy_tree('/content/drive/MyDrive/TFM/simulations', '/content/simulations')\n",
        "\n",
        "    # unzip all simulations\n",
        "    print(\"Name of the already simulated data: \\n\", )\n",
        "    for zip_file in os.listdir('/content/simulations'):\n",
        "        if zip_file.endswith('.zip'):\n",
        "            print(\" ----> \" + zip_file)\n",
        "            with zipfile.ZipFile(\"/content/simulations/\" + zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content/simulations/')\n",
        "\n",
        "    SIMULATIONS_PATH: str = \"/content/simulations\"\n",
        "else:\n",
        "    if not os.path.exists('./data/simulations'): os.mkdir('./data/simulations')\n",
        "    for zip_file in os.listdir('data'):\n",
        "        if zip_file.endswith('.zip'):\n",
        "            print(\" ----> \" + zip_file)\n",
        "            with zipfile.ZipFile(\"./data/\" + zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall('./data/simulations')\n",
        "    SIMULATIONS_PATH: str = \"./data/simulations\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMKFCdf_s4Bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f01f7b5-38db-4b0d-c48b-a8238c935806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channel    matrix shape: 100000 x  4 x 20 x 20 \n",
            "allocation matrix shape: 100000 x 20\n"
          ]
        }
      ],
      "source": [
        "cmg   = np.load(SIMULATIONS_PATH + '/Channel_matrix_gain.npy')\n",
        "sisa_alloc = np.load(SIMULATIONS_PATH + '/sisa-allocation.npy')\n",
        "\n",
        "# get sample from all\n",
        "n_sample = 100_000\n",
        "cmg   = cmg[:n_sample]\n",
        "sisa_alloc = sisa_alloc[:n_sample]\n",
        "\n",
        "n_sample = cmg.shape[0]\n",
        "K, N, _  = cmg.shape[1:]\n",
        "\n",
        "shape    = lambda s: \" x\".join([f\"{d:3d}\" for d in s])\n",
        "print(f\"channel    matrix shape: {shape(cmg.shape)} \\nallocation matrix shape: {shape(sisa_alloc.shape)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4rgJ-N0s4Bh"
      },
      "source": [
        "## Publications to revise\n",
        "\n",
        "* (power) Power control for 6g industrial wireless subnetworks: A graph neural network approach\n",
        "* (allocation) Towards 6g in-x subnetworks with sub-millisecond communication cycles and extreme reliability\n",
        "* (power) Multi-agent deep reinforcement learning for dynamic power allocation in wireless networks\n",
        "* (both) Multi-agent reinforcement learning for dynamic resource management in 6g in-x subnetworks\n",
        "* (both) Multi-agent dynamic resource allocation in 6g in-x subnetworks with limited sensing information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F--fEzMs4Bi"
      },
      "source": [
        "## First Proposal\n",
        "\n",
        "In this proposal, we could mixed different implementations for optimization of problem.\n",
        "We can only consider the following setup:\n",
        "\n",
        "1. Determine a almost optimal subband allocation for the networks. We could use a power selection of $p = p_{max}$\n",
        "2. Based on the obtained allocation, we determine a power control for each subnetwork that minimizes the used\n",
        "power and does not deteriorite the signal.\n",
        "\n",
        "For the subband allocation, we could consider the implementation from this [publication](https://ieeexplore.ieee.org/document/10597067).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Using DNN Architecture"
      ],
      "metadata": {
        "id": "P9wQ8SUPJFFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SIFtDfIs4Bi"
      },
      "outputs": [],
      "source": [
        "# First Step: Subband allocation problem\n",
        "class RateConfirmAllocModel(nn.Module):\n",
        "    def __init__(self, n_subnetworks: int, n_bands: int,\n",
        "                 hidden_dim: int = 1000, hidden_layers: int = 4,\n",
        "                 dropout: float | None = 0.01,\n",
        "                 use_weighted: bool = False,\n",
        "                 keep_band_wise: bool = False) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize state\n",
        "        self.n = n_subnetworks\n",
        "        self.k = n_bands\n",
        "\n",
        "        # preprocessing options\n",
        "        self.use_weighted   = use_weighted\n",
        "        self.keep_band_wise = keep_band_wise\n",
        "\n",
        "        # DNN architecture\n",
        "        self.input_size = self.n * self.n if not self.keep_band_wise else self.n * self.n * self.k\n",
        "        self.output_size = self.n * self.k\n",
        "\n",
        "        last_layer = -2 if dropout is None else -3\n",
        "        layers = [nn.BatchNorm1d(self.input_size)] # with batch norm at start\n",
        "        dims = [self.input_size] + [hidden_dim] * (hidden_layers + 1) + [self.output_size]\n",
        "        for i in range(1, len(dims)):\n",
        "            # linear layers with HE initialization\n",
        "            layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
        "            torch.nn.init.kaiming_normal_(layers[-1].weight, nonlinearity='relu')\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "            # apply dropout. We have a lot of parameters, it is required\n",
        "            layers.append(nn.BatchNorm1d(dims[i]))\n",
        "            if isinstance(dropout, float):\n",
        "              layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        layers = layers[:last_layer]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def preprocess(self, channel_gain: np.ndarray | torch.Tensor ) -> torch.Tensor:\n",
        "        channel_gain = torch.tensor(channel_gain, requires_grad=False).float()\n",
        "\n",
        "        if self.keep_band_wise and len(channel_gain.shape[1:]) != 3:\n",
        "            raise ValueError(\"The model expects a channel gain matrix (BxKxNxN)\")\n",
        "\n",
        "        if self.keep_band_wise and self.use_weighted:\n",
        "            # normalize the interference matrix\n",
        "            Hd = torch.diagonal(channel_gain, dim1 = -2, dim2 = -1).unsqueeze(-1)\n",
        "\n",
        "            zero = torch.zeros_like(channel_gain).to(channel_gain.device)\n",
        "            channel_gain = torch.where(Hd > 0, channel_gain / Hd, zero)\n",
        "\n",
        "            # remove self-interference\n",
        "            B, K, N, _ = channel_gain.shape\n",
        "            self_signal = torch.eye(N, device = device).expand(B, K, -1, -1)\n",
        "            channel_gain = channel_gain * (1 - self_signal)\n",
        "\n",
        "        elif not self.keep_band_wise and len(channel_gain.shape[1:]) == 3:\n",
        "            channel_gain = torch.mean(channel_gain, dim = 1)\n",
        "\n",
        "        # flatten the channel information\n",
        "        channel_gain = channel_gain.flatten(start_dim=1)\n",
        "\n",
        "        # transform to dbm scale to restrict value range\n",
        "        channel_gain = 10 * torch.log10(channel_gain + 1e-9) # transform to Dbm\n",
        "\n",
        "        # normalize to values.\n",
        "        # cavg = channel_gain.mean(dim = 1, keepdim = True)\n",
        "        # cstd = channel_gain.std( dim = 1, keepdim = True)\n",
        "        # channel_gain = (channel_gain - cavg) / cstd\n",
        "        return channel_gain\n",
        "\n",
        "    def forward(self, channel_gain: torch.Tensor, t: float = 1.0 ) -> torch.Tensor:\n",
        "        # preprocess to obtain a NxN channel gain\n",
        "        channel_gain = self.preprocess(channel_gain)\n",
        "        # apply model\n",
        "        channel_network = self.model(channel_gain)\n",
        "        # determine best allocation\n",
        "        channel_network = channel_network.reshape(-1, self.k, self.n)\n",
        "        # derive probabilities\n",
        "        return F.softmax(channel_network / t, dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEiBBXrys4Bj"
      },
      "source": [
        "The used loss function in the first-stage correspond to modified version of:\n",
        "\n",
        "$$ \\sum \\mathbb{E}(\\hat{R} > R^{REQ}) $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbIFu3a4s4Bj"
      },
      "outputs": [],
      "source": [
        "def loss_fullfield_req(config: SimConfig, C: torch.Tensor, A: torch.Tensor, req: float, mode: str = 'mean') -> torch.Tensor:\n",
        "    # calculate shannon rate\n",
        "    sinr = rate_metrics.signal_interference_ratio(config, C, A, None)\n",
        "    rate = torch.sum(A * torch.log2(1 + sinr), dim = 1)\n",
        "\n",
        "    rate = F.sigmoid(req - rate) / req\n",
        "    rate = torch.sum(rate, dim=1)\n",
        "    return rate\n",
        "\n",
        "def min_approx(x: torch.Tensor, p: float = 1e2):\n",
        "    \"\"\"\n",
        "    Differentiable Approximation of Minimum Function. This function approximates\n",
        "    the value of min(x)\n",
        "\n",
        "      # based on fC https://mathoverflow.net/questions/35191/a-differentiable-approximation-to-the-minimum-function\n",
        "    \"\"\"\n",
        "    mu = 0\n",
        "    inner = torch.mean(torch.exp(- p * (x - mu)), dim = 1)\n",
        "    return mu - (1 / p) * torch.log(inner)\n",
        "\n",
        "def loss_pure_rate(config: SimConfig, C: torch.Tensor, A: torch.Tensor, mode: str = 'sum', p: int = 10) -> torch.Tensor:\n",
        "    sinr = rate_metrics.signal_interference_ratio(config, C, A, None)\n",
        "    rate = torch.sum(A * torch.log2(1 + sinr), dim = 1)\n",
        "\n",
        "    if mode == 'sum':\n",
        "      loss_rate = torch.sum(rate, dim = 1)\n",
        "    elif mode == 'min':\n",
        "      loss_rate = min_approx(rate, p)\n",
        "    elif mode == 'max':\n",
        "      loss_rate = torch.sum(rate, dim = 1)\n",
        "    return - loss_rate\n",
        "\n",
        "class TemperatureScheduler:\n",
        "    def __init__(self, tinit=1.0, gamma=0.99, tmin = 1e-10):\n",
        "        self.tinit = tinit\n",
        "        self.gamma = gamma\n",
        "        self.tmin  = tmin\n",
        "        self.step_count = 0\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Increases tau based on an exponential schedule.\"\"\"\n",
        "        temp = self.tinit * self.gamma ** self.step_count\n",
        "        self.step_count += 1\n",
        "        return max(temp, self.tmin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ7OzEmGuZ9v"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "def real_time_plot(*metrics):\n",
        "    names = ['training', 'validation']\n",
        "    assert len(metrics) % 2 == 0, \"A odd pair of metrics is required\"\n",
        "    clear_output(wait=True)  # Clear the previous plot\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 4))  # Two subplots, stacked vertically\n",
        "    # Plot loss\n",
        "    for i, loss in enumerate(metrics[:len(metrics) // 2]):\n",
        "      ax[0].plot(loss, label = f\"loss: {names[i]}\")\n",
        "    ax[0].set_title('Real-Time Loss')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].legend()\n",
        "\n",
        "    # Plot metric (e.g., SINR or accuracy)\n",
        "    for i, metric in enumerate(metrics[len(metrics) // 2:]):\n",
        "      ax[1].plot(metric, label = f\"loss: {names[i]}\")\n",
        "    ax[1].set_title('Real-Time Metric')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_ylabel('Bit Rate (Mbps)')\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.tight_layout()  # Adjust layout to avoid overlap\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ak0CcPu2i1"
      },
      "outputs": [],
      "source": [
        "def binarization_error(alloc: torch.Tensor) -> float:\n",
        "    rounded = torch.round(alloc)\n",
        "    return torch.mean(torch.abs(alloc - rounded))\n",
        "\n",
        "def update_metrics(metrics, A, C, config, req):\n",
        "    A    = rate_metrics.onehot_allocation(A, 4, 20)\n",
        "    sinr = rate_metrics.signal_interference_ratio(config, C, A, None)\n",
        "    rate = rate_metrics.bit_rate(config, sinr, A)\n",
        "    fairness = rate_metrics.jain_fairness(rate)\n",
        "    spectral = rate_metrics.spectral_efficency(config, rate)\n",
        "    plf      = rate_metrics.proportional_loss_factor(config, C, A, None)\n",
        "\n",
        "    shannon  = torch.sum(A * torch.log2(1 + sinr), dim = 1)\n",
        "    ecf_req  = torch.mean((shannon >= req).float(), dim = 1)\n",
        "\n",
        "    metrics['bit-rate'] += rate.mean().item() / 1e6\n",
        "    metrics['jain-fairness'] += fairness.mean().item()\n",
        "    metrics['spectral-efficency'] += spectral.mean().item()\n",
        "    metrics['proportional-loss' ] += plf.mean().item()\n",
        "    metrics['over-requirement' ] += ecf_req.mean().item()\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29JxMoLsxXUo"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE: int = 1024\n",
        "\n",
        "# build datasets\n",
        "TRAIN_SAMPLE: int =  75_000\n",
        "VALID_SAMPLE: int =  25_000\n",
        "\n",
        "#TESTS_SAMPLE: int =   0_000\n",
        "whole_data = TensorDataset(torch.tensor(cmg).float())\n",
        "train_data, valid_data = random_split(\n",
        "    whole_data,\n",
        "    [TRAIN_SAMPLE, VALID_SAMPLE], # , TESTS_SAMPLE\n",
        "    torch.Generator().manual_seed(101)\n",
        ")\n",
        "\n",
        "train_data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle = True)\n",
        "valid_data = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle = True)\n",
        "# tests_data = DataLoader(tests_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl2Xj5FQs4Bk"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "raise NotImplementedError(\"this model was already tested, we will skip it\")\n",
        "\n",
        "BATCH_SIZE: int = 1024\n",
        "MAX_EPOCH : int = 200\n",
        "LR: float  = 1e-4\n",
        "\n",
        "# under ideal conditions, the sisa ideal shannon rate is around 4.\n",
        "REQ: float      = 8.\n",
        "\n",
        "learning_config = {\n",
        "    'loss': 'pure-min-rate',\n",
        "    'max-epoch': MAX_EPOCH,\n",
        "    'batch-size': BATCH_SIZE,\n",
        "    'learning-rate': LR,\n",
        "    'desired-norm-rate' : REQ,\n",
        "    'train-valid-split' : f\"{TRAIN_SAMPLE}-{VALID_SAMPLE}\"\n",
        "}\n",
        "\n",
        "# training config\n",
        "HS: int    = 1024\n",
        "HL: int    = 6\n",
        "DP: float  = 0.1\n",
        "KEEP_BANDS: bool = True\n",
        "WEIGHTED_GAIN: bool = True\n",
        "\n",
        "model_config = {\n",
        "    'hidden-dim': HS,\n",
        "    'hidden-layers': HL,\n",
        "    'keep-bands': KEEP_BANDS,\n",
        "    'weighted-gain': WEIGHTED_GAIN,\n",
        "}\n",
        "\n",
        "name  = \"p1-alloc-dnn-03-00-base\"\n",
        "training_config = {}\n",
        "training_config.update(model_config)\n",
        "training_config.update(learning_config)\n",
        "\n",
        "try: wandb.finish(quiet = True)\n",
        "except: pass\n",
        "run = setup_wandb(name, 'rate-confirming', training_config, id = None)\n",
        "print(\"run config:\", run.config)\n",
        "\n",
        "model = RateConfirmAllocModel(20, 4, HS, HL, DP, KEEP_BANDS, WEIGHTED_GAIN).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), LR, weight_decay=1e-5)\n",
        "scheduler = lrs.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
        "temp_scheduler = TemperatureScheduler(1.0, 0.98)\n",
        "\n",
        "train_loss, valid_loss, train_rate, valid_rate = [], [], [], []\n",
        "for epoch in trange(MAX_EPOCH, desc = \"training epoch\", unit = \"epoch\"):\n",
        "    real_time_plot(train_loss, valid_loss, train_rate, valid_rate)\n",
        "\n",
        "    # training step\n",
        "    model.train()\n",
        "    training_loss = 0.\n",
        "    train_binary_loss = 0.\n",
        "\n",
        "    temp = 1.0 # temp_scheduler.step()\n",
        "    training_metrics = defaultdict(lambda : 0)\n",
        "    for sample in tqdm(train_data, desc = 'training step:', unit = 'batch', total = len(train_data), leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sample     = sample[0].to(device)\n",
        "        alloc_prob = model(sample, temp)\n",
        "        loss       = loss_pure_rate(config, sample, alloc_prob, 'min').mean()\n",
        "        # loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_binary_loss += binarization_error(alloc_prob).item()\n",
        "        training_metrics = update_metrics(training_metrics, alloc_prob, sample, config, REQ)\n",
        "\n",
        "    scheduler.step()\n",
        "    training_loss = training_loss / len(train_data)\n",
        "    train_binary_loss = train_binary_loss / len(train_data)\n",
        "    training_metrics = { 'train-' + key: val / len(train_data) for key, val in training_metrics.items()}\n",
        "\n",
        "    model.eval()\n",
        "    validation_loss = 0.\n",
        "    valid_binary_loss = 0.\n",
        "    validation_metrics = defaultdict(lambda : 0.)\n",
        "    for sample in tqdm(valid_data, desc = 'validation step:', unit = 'batch', total = len(valid_data), leave = False):\n",
        "        sample     = sample[0].to(device)\n",
        "        alloc_prob = model(sample, temp)\n",
        "        loss       = loss_pure_rate(config, sample, alloc_prob, 'min').mean()\n",
        "        # loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
        "        validation_loss += loss.item()\n",
        "\n",
        "        valid_binary_loss += binarization_error(alloc_prob).item()\n",
        "        validation_metrics = update_metrics(validation_metrics, alloc_prob, sample, config, REQ)\n",
        "\n",
        "    validation_loss = validation_loss / len(valid_data)\n",
        "    valid_binary_loss = valid_binary_loss / len(valid_data)\n",
        "\n",
        "    validation_metrics = { 'valid-' + key: val / len(valid_data) for key, val in validation_metrics.items()}\n",
        "\n",
        "    logged_values = {\n",
        "        'train-loss': training_loss, 'valid-loss': validation_loss, 'temperature': temp,\n",
        "        'train-binary-loss': train_binary_loss, 'valid-binary-loss': valid_binary_loss\n",
        "    }\n",
        "\n",
        "    logged_values.update(training_metrics)\n",
        "    logged_values.update(validation_metrics)\n",
        "\n",
        "    train_loss.append(training_loss)\n",
        "    valid_loss.append(validation_loss)\n",
        "    train_rate.append(training_metrics['train-bit-rate'])\n",
        "    valid_rate.append(validation_metrics['valid-bit-rate'])\n",
        "    wandb.log(logged_values)\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "sisa_tensor = torch.tensor(sisa_alloc[-40_000:]).type(torch.int64).to(device)\n",
        "data_tensor = torch.tensor(cmg[-40_000:]).to(device)\n",
        "allc_tensor = model(data_tensor).to(device)\n",
        "\n",
        "print(sisa_tensor.shape, data_tensor.shape, allc_tensor.shape)\n",
        "print(update_metrics(defaultdict(lambda : 0), sisa_tensor, data_tensor, config, 6))\n",
        "print(update_metrics(defaultdict(lambda : 0), allc_tensor, data_tensor, config, 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Using CNN Architecture\n",
        "\n",
        " THe previos DNN fails to consider possible relationships between subbands and the other subnetworks. We can leverage CNN architecture to do that."
      ],
      "metadata": {
        "id": "XnjGsHlCwV8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvifQXKKRuil"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def create_cnn_block( in_channels: int, out_channels: int,\n",
        "                      kernel_size: int, pad_size: int = 0, stride_size: int = 1,\n",
        "                      dropout: float | None = None, norm: bool = True\n",
        "                    ) -> nn.Sequential:\n",
        "\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride_size, pad_size)]\n",
        "\n",
        "    if norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    layers.append(nn.ReLU())\n",
        "\n",
        "    if dropout is not None:\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def calculate_flatten_size(input_size: tuple[int, int, int, int], cnn_layers: list[dict[str, int]]) -> tuple[tuple[int, int, int, int], int]:\n",
        "\n",
        "    B, C, H, W = input_size  # Unpack initial dimensions\n",
        "\n",
        "    for layer in cnn_layers:\n",
        "        if layer[\"type\"] == \"conv\":\n",
        "            # Apply convolution formula\n",
        "            H = (H + 2 * layer[\"padding\"] - layer[\"kernel_size\"]) // layer[\"stride\"] + 1\n",
        "            W = (W + 2 * layer[\"padding\"] - layer[\"kernel_size\"]) // layer[\"stride\"] + 1\n",
        "            C = layer[\"out_channels\"]  # Update channel count\n",
        "\n",
        "        elif layer[\"type\"] == \"pool\":\n",
        "            # Apply pooling formula (assuming stride = pool_size for simplicity)\n",
        "            H //= layer[\"pool_size\"]\n",
        "            W //= layer[\"pool_size\"]\n",
        "\n",
        "        elif layer[\"type\"] == \"flatten\":\n",
        "            # Stop tracking spatial dimensions, return final shape\n",
        "            return (B, C, H, W), C * H * W\n",
        "\n",
        "    return (B, C, H, W), C * H * W\n",
        "\n",
        "# Example Usage\n",
        "input_size = (B := 1, C := 3, H := 32, W := 32)  # Example input: (Batch=1, Channels=3, 32x32 image)\n",
        "\n",
        "\n",
        "# First Step: Subband allocation problem\n",
        "class RateConfirmAllocCNNModel(nn.Module):\n",
        "    def __init__(self, n_subnetworks: int, n_bands: int,\n",
        "                 dropout: float | None = 0.01,\n",
        "                 use_weighted: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize state\n",
        "        self.n = n_subnetworks\n",
        "        self.k = n_bands\n",
        "\n",
        "        # preprocessing options\n",
        "        self.use_weighted   = use_weighted\n",
        "\n",
        "        # DNN architecture\n",
        "        self.output_size = self.n * self.k\n",
        "\n",
        "        last_layer = -2 if dropout is None else -3\n",
        "        layers = [nn.BatchNorm2d(n_bands)] # with batch norm at start\n",
        "\n",
        "\n",
        "        # add CNN blocks\n",
        "        layers.append(create_cnn_block(n_bands, 16,  3, 1, 1, dropout, True))\n",
        "        layers.append(create_cnn_block(16, 64,  3, 0, 1, dropout, True))\n",
        "        layers.append(nn.AvgPool2d(2))\n",
        "        layers.append(create_cnn_block(64, 256, 3, 0, 1, dropout, True))\n",
        "        layers.append(nn.AvgPool2d(2))\n",
        "        layers.append(nn.Flatten())\n",
        "\n",
        "        self.cnn_layer_info = [\n",
        "            {\"type\": \"conv\", \"out_channels\": 16, \"kernel_size\": 3, \"padding\": 1, \"stride\": 1},\n",
        "            {\"type\": \"conv\", \"out_channels\": 64, \"kernel_size\": 3, \"padding\": 0, \"stride\": 1},\n",
        "            {\"type\": \"pool\", \"pool_size\": 2},  # AvgPool2d(2)\n",
        "            {\"type\": \"conv\", \"out_channels\": 256, \"kernel_size\": 3, \"padding\": 0, \"stride\": 1},\n",
        "            {\"type\": \"pool\", \"pool_size\": 2},  # AvgPool2d(2)\n",
        "            {\"type\": \"flatten\"},\n",
        "        ]\n",
        "        flatten_size = calculate_flatten_size((1, n_bands, n_subnetworks, n_subnetworks), self.cnn_layer_info)[1]\n",
        "        # add FNN blocks\n",
        "\n",
        "        dims = [flatten_size] + [1024, 512] + [self.output_size]\n",
        "        for i in range(1, len(dims)):\n",
        "            # linear layers with HE initialization\n",
        "            layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
        "            torch.nn.init.kaiming_normal_(layers[-1].weight, nonlinearity='relu')\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "            # apply dropout. We have a lot of parameters, it is required\n",
        "            layers.append(nn.BatchNorm1d(dims[i]))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        layers = layers[:-3]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def preprocess(self, channel_gain: np.ndarray | torch.Tensor ) -> torch.Tensor:\n",
        "        channel_gain = torch.tensor(channel_gain, requires_grad=False).float()\n",
        "        device       = channel_gain.device\n",
        "        if self.use_weighted:\n",
        "            # normalize the interference matrix\n",
        "            Hd = torch.diagonal(channel_gain, dim1 = -2, dim2 = -1).unsqueeze(-1)\n",
        "\n",
        "            zero = torch.zeros_like(channel_gain).to(channel_gain.device)\n",
        "            channel_gain = torch.where(Hd > 0, channel_gain / Hd, zero)\n",
        "\n",
        "            # remove self-interference\n",
        "            B, K, N, _ = channel_gain.shape\n",
        "            self_signal = torch.eye(N, device = device).expand(B, K, -1, -1)\n",
        "            channel_gain = channel_gain * (1 - self_signal)\n",
        "\n",
        "        # transform to dbm scale to restrict value range\n",
        "        channel_gain = 10 * torch.log10(channel_gain + 1e-9) # transform to Dbm\n",
        "\n",
        "        # normalize to values.\n",
        "        # cavg = channel_gain.mean(dim = 1, keepdim = True)\n",
        "        # cstd = channel_gain.std( dim = 1, keepdim = True)\n",
        "        # channel_gain = (channel_gain - cavg) / cstd\n",
        "        return channel_gain\n",
        "\n",
        "    def forward(self, channel_gain: torch.Tensor, t: float = 1.0 ) -> torch.Tensor:\n",
        "        # preprocess to obtain a NxN channel gain\n",
        "        channel_gain = self.preprocess(channel_gain)\n",
        "        # apply model\n",
        "        channel_network = self.model(channel_gain)\n",
        "        # determine best allocation\n",
        "        channel_network = channel_network.reshape(-1, self.k, self.n)\n",
        "        # derive probabilities\n",
        "        return F.softmax(channel_network / t, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "BATCH_SIZE: int = 1024\n",
        "MAX_EPOCH : int = 80\n",
        "LR: float  = 3e-4\n",
        "\n",
        "# under ideal conditions, the sisa ideal shannon rate is around 4.\n",
        "REQ: float      = 8.\n",
        "\n",
        "learning_config = {\n",
        "    'loss': 'pure-min-rate',\n",
        "    'max-epoch': MAX_EPOCH,\n",
        "    'batch-size': BATCH_SIZE,\n",
        "    'learning-rate': LR,\n",
        "    'desired-norm-rate' : REQ,\n",
        "    'train-valid-split' : f\"{TRAIN_SAMPLE}-{VALID_SAMPLE}\"\n",
        "}\n",
        "\n",
        "# training config\n",
        "HS: int    = 1024\n",
        "HL: int    = 6\n",
        "DP: float  = 0.1\n",
        "KEEP_BANDS: bool = True\n",
        "WEIGHTED_GAIN: bool = True\n",
        "\n",
        "model_config = {\n",
        "    'dropout': DP,\n",
        "    'weighted-gain': WEIGHTED_GAIN,\n",
        "}\n",
        "\n",
        "name  = \"p1-alloc-cnn-00-01-base\"\n",
        "training_config = {}\n",
        "training_config.update(model_config)\n",
        "training_config.update(learning_config)\n",
        "\n",
        "try: wandb.finish(quiet = True)\n",
        "except: pass\n",
        "run = setup_wandb(name, 'cnn-rate-confirming', training_config, id = None)\n",
        "print(\"run config:\", run.config)\n",
        "\n",
        "model = RateConfirmAllocCNNModel(20, 4, DP, WEIGHTED_GAIN).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), LR, weight_decay=1e-5)\n",
        "scheduler = lrs.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-4)\n",
        "temp_scheduler = TemperatureScheduler(1.0, 0.98)\n",
        "\n",
        "train_loss, valid_loss, train_rate, valid_rate = [], [], [], []\n",
        "for epoch in trange(MAX_EPOCH, desc = \"training epoch\", unit = \"epoch\"):\n",
        "    real_time_plot(train_loss, valid_loss, train_rate, valid_rate)\n",
        "\n",
        "    # training step\n",
        "    model.train()\n",
        "    training_loss = 0.\n",
        "    train_binary_loss = 0.\n",
        "\n",
        "    temp = 1.0 # temp_scheduler.step()\n",
        "    training_metrics = defaultdict(lambda : 0)\n",
        "    for sample in tqdm(train_data, desc = 'training step:', unit = 'batch', total = len(train_data), leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sample     = sample[0].to(device)\n",
        "        alloc_prob = model(sample, temp)\n",
        "        loss       = loss_pure_rate(config, sample, alloc_prob, 'min').mean()\n",
        "        # loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_binary_loss += binarization_error(alloc_prob).item()\n",
        "        training_metrics = update_metrics(training_metrics, alloc_prob, sample, config, REQ)\n",
        "\n",
        "    scheduler.step()\n",
        "    training_loss = training_loss / len(train_data)\n",
        "    train_binary_loss = train_binary_loss / len(train_data)\n",
        "    training_metrics = { 'train-' + key: val / len(train_data) for key, val in training_metrics.items()}\n",
        "\n",
        "    model.eval()\n",
        "    validation_loss = 0.\n",
        "    valid_binary_loss = 0.\n",
        "    validation_metrics = defaultdict(lambda : 0.)\n",
        "    for sample in tqdm(valid_data, desc = 'validation step:', unit = 'batch', total = len(valid_data), leave = False):\n",
        "        sample     = sample[0].to(device)\n",
        "        alloc_prob = model(sample, temp)\n",
        "        loss       = loss_pure_rate(config, sample, alloc_prob, 'min').mean()\n",
        "        # loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
        "        validation_loss += loss.item()\n",
        "\n",
        "        valid_binary_loss += binarization_error(alloc_prob).item()\n",
        "        validation_metrics = update_metrics(validation_metrics, alloc_prob, sample, config, REQ)\n",
        "\n",
        "    validation_loss = validation_loss / len(valid_data)\n",
        "    valid_binary_loss = valid_binary_loss / len(valid_data)\n",
        "\n",
        "    validation_metrics = { 'valid-' + key: val / len(valid_data) for key, val in validation_metrics.items()}\n",
        "\n",
        "    logged_values = {\n",
        "        'train-loss': training_loss, 'valid-loss': validation_loss, 'temperature': temp,\n",
        "        'train-binary-loss': train_binary_loss, 'valid-binary-loss': valid_binary_loss,\n",
        "        'learning-rate': scheduler.get_lr()[0]\n",
        "    }\n",
        "\n",
        "    logged_values.update(training_metrics)\n",
        "    logged_values.update(validation_metrics)\n",
        "\n",
        "    train_loss.append(training_loss)\n",
        "    valid_loss.append(validation_loss)\n",
        "    train_rate.append(training_metrics['train-bit-rate'])\n",
        "    valid_rate.append(validation_metrics['valid-bit-rate'])\n",
        "    wandb.log(logged_values)\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "sisa_tensor = torch.tensor(sisa_alloc[-30_000:]).type(torch.int64).to(device)\n",
        "data_tensor = torch.tensor(cmg[-30_000:]).to(device)\n",
        "allc_tensor = model(data_tensor).to(device)\n",
        "\n",
        "print(sisa_tensor.shape, data_tensor.shape, allc_tensor.shape)\n",
        "print(update_metrics(defaultdict(lambda : 0), sisa_tensor, data_tensor, config, 6))\n",
        "print(update_metrics(defaultdict(lambda : 0), allc_tensor, data_tensor, config, 6))\n"
      ],
      "metadata": {
        "id": "DyBDlwkEF738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_model_size(model: nn.Module):\n",
        "  param_size = 0\n",
        "  for param in model.parameters():\n",
        "      param_size += param.nelement() * param.element_size()\n",
        "  buffer_size = 0\n",
        "  for buffer in model.buffers():\n",
        "      buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "  size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "  return size_all_mb\n"
      ],
      "metadata": {
        "id": "tzKFLt2ceMyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Graph Based Approach"
      ],
      "metadata": {
        "id": "p7gTVTzOozXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_large_graph(channel_gain: torch.Tensor) -> Data:\n",
        "    B, K, N, _ = channel_gain.shape\n",
        "    device = channel_gain.device\n",
        "    total_nodes = B * N  # All nodes from all batches\n",
        "\n",
        "    # Compute node features\n",
        "    interference = channel_gain.mean(dim=1)  # Average over K\n",
        "    node_features = interference.mean(dim=1)  # Average over N\n",
        "\n",
        "    print(\"creatiing edges\", node_features.shape)\n",
        "\n",
        "    # Mask diagonal elements (self-interference)\n",
        "    mask = torch.eye(N, device=device).bool()\n",
        "    interference.masked_fill_(mask[None, :, :], float('-inf'))\n",
        "\n",
        "    # Find top K-1 strongest interfering nodes per batch\n",
        "    neighbors = torch.topk(interference, k=K-1, dim=-1).indices\n",
        "\n",
        "    # Generate global node indices\n",
        "    batch_offset = torch.arange(B, device=device)[:, None] * N  # Offset for batch indexing\n",
        "    global_src_nodes = (torch.arange(N, device=device).repeat(B, K-1).view(B, N, K-1) + batch_offset[:, None, None]).flatten()\n",
        "    global_neighbors = (neighbors + batch_offset[:, None, None]).flatten()\n",
        "\n",
        "    print(\"creatiing edges\")\n",
        "\n",
        "    # Construct edge index\n",
        "    edge_index = torch.stack([global_src_nodes, global_neighbors], dim=0)\n",
        "    valid_edges = edge_index[0] != edge_index[1]  # Remove self-loops\n",
        "    edge_index = edge_index[:, valid_edges]\n",
        "\n",
        "    # Node features for all nodes\n",
        "    node_features = node_features.view(-1, 1)  # Reshape to (B*N, 1)\n",
        "\n",
        "    # Create training mask (70% of batches, ensuring full batches are selected)\n",
        "    train_mask = torch.zeros(total_nodes, dtype=torch.bool, device=device)\n",
        "    num_train_batches = int(0.7 * B)\n",
        "    selected_batches = torch.randperm(B, device=device)[:num_train_batches]\n",
        "    batch_mask = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "    batch_mask[selected_batches] = True\n",
        "    train_mask = batch_mask.repeat_interleave(N)\n",
        "\n",
        "    return Data(x=node_features, edge_index=edge_index, train_mask=train_mask)\n",
        "\n",
        "train_data, _ = random_split( whole_data, [TRAIN_SAMPLE, VALID_SAMPLE] )\n",
        "graphs = create_large_graph(train_data.dataset.tensors[0])\n",
        "print(graphs.x.shape)\n"
      ],
      "metadata": {
        "id": "kVVzYbx0o2L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c76180b-2d35-4211-b5e3-1446515fffa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creatiing edges torch.Size([100000, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Mg6g4VxNbDK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
        "class RateConfirmGNNModel(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "# GCN layers\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            GCNConv(in_dim, hidden_dim) if i == 0 else GCNConv(hidden_dim, hidden_dim)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Ensure that the data is a batch\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # GCN Layers\n",
        "        for conv in self.conv_layers:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "\n",
        "        # GRU layer (processing sequential dependencies)\n",
        "        # GRU expects input shape: (batch_size, seq_len, input_size)\n",
        "        # Here, seq_len = num_nodes, input_size = hidden_dim after the GCN layer\n",
        "        x = x.view(-1, x.size(1), x.size(2))  # (B * N, hidden_dim) -> (B, N, hidden_dim)\n",
        "        x, _ = self.gru(x)  # x: (B, N, hidden_dim)\n",
        "\n",
        "        # Readout operation: take the node features and pass through the output layer\n",
        "        x = x.mean(dim=1)  # Pooling: global mean of node features\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc_out(x)  # Output shape: (B, out_dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "model = RateConfirmGNNModel(in_dim=1, hidden_dim=64, out_dim=4)  # K is the number of subbands\n",
        "graphs = create_graphs(next(iter(train_data))[0])\n",
        "\n",
        "model.eval()\n",
        "model(graphs[0])\n"
      ],
      "metadata": {
        "id": "RnJjwY8bKhbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE: int = 1024\n",
        "MAX_EPOCH : int = 80\n",
        "LR: float  = 3e-4\n",
        "\n",
        "# under ideal conditions, the sisa ideal shannon rate is around 4.\n",
        "REQ: float      = 8.\n",
        "\n",
        "learning_config = {\n",
        "    'loss': 'pure-min-rate',\n",
        "    'max-epoch': MAX_EPOCH,\n",
        "    'batch-size': BATCH_SIZE,\n",
        "    'learning-rate': LR,\n",
        "    'desired-norm-rate' : REQ,\n",
        "    'train-valid-split' : f\"{TRAIN_SAMPLE}-{VALID_SAMPLE}\"\n",
        "}\n",
        "\n",
        "# training config\n",
        "HS: int    = 1024\n",
        "DP: float  = 0.1\n",
        "\n",
        "model_config = {\n",
        "    'dropout': DP,\n",
        "    'hidden-dim': HS,\n",
        "}\n",
        "\n",
        "name  = \"p1-alloc-gnn-00-01-base\"\n",
        "training_config = {}\n",
        "training_config.update(model_config)\n",
        "training_config.update(learning_config)\n",
        "\n",
        "try: wandb.finish(quiet = True)\n",
        "except: pass\n",
        "run = setup_wandb(name, 'gnn-rate-confirming', training_config, id = None)\n",
        "print(\"run config:\", run.config)\n",
        "\n",
        "model = SubbandAllocationGNN(4, HS, 20, 4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), LR, weight_decay=1e-5)\n",
        "scheduler = lrs.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-4)\n",
        "temp_scheduler = TemperatureScheduler(1.0, 0.98)\n",
        "\n",
        "train_loss, valid_loss, train_rate, valid_rate = [], [], [], []\n",
        "for epoch in trange(MAX_EPOCH, desc = \"training epoch\", unit = \"epoch\"):\n",
        "    real_time_plot(train_loss, valid_loss, train_rate, valid_rate)\n",
        "\n",
        "    # training step\n",
        "    model.train()\n",
        "    training_loss = 0.\n",
        "    train_binary_loss = 0.\n",
        "\n",
        "    temp = 1.0 # temp_scheduler.step()\n",
        "    training_metrics = defaultdict(lambda : 0)\n",
        "    for sample in tqdm(train_data, desc = 'training step:', unit = 'batch', total = len(train_data), leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sample     = sample[0].to(device)\n",
        "        alloc_prob = model(sample)\n",
        "        loss       = loss_pure_rate(config, sample, alloc_prob, 'min').mean()\n",
        "        # loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_binary_loss += binarization_error(alloc_prob).item()\n",
        "        training_metrics = update_metrics(training_metrics, alloc_prob, sample, config, REQ)\n",
        "\n",
        "    scheduler.step()\n",
        "    training_loss = training_loss / len(train_data)\n",
        "    train_binary_loss = train_binary_loss / len(train_data)\n",
        "    training_metrics = { 'train-' + key: val / len(train_data) for key, val in training_metrics.items()}\n",
        "\n",
        "    model.eval()\n",
        "    validation_loss = 0.\n",
        "    valid_binary_loss = 0.\n",
        "    validation_metrics = defaultdict(lambda : 0.)\n",
        "    for sample in tqdm(valid_data, desc = 'validation step:', unit = 'batch', total = len(valid_data), leave = False):\n",
        "        sample     = sample[0].to(device)\n",
        "        alloc_prob = model(sample)\n",
        "        loss       = loss_pure_rate(config, sample, alloc_prob, 'min').mean()\n",
        "        # loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
        "        validation_loss += loss.item()\n",
        "\n",
        "        valid_binary_loss += binarization_error(alloc_prob).item()\n",
        "        validation_metrics = update_metrics(validation_metrics, alloc_prob, sample, config, REQ)\n",
        "\n",
        "    validation_loss = validation_loss / len(valid_data)\n",
        "    valid_binary_loss = valid_binary_loss / len(valid_data)\n",
        "\n",
        "    validation_metrics = { 'valid-' + key: val / len(valid_data) for key, val in validation_metrics.items()}\n",
        "\n",
        "    logged_values = {\n",
        "        'train-loss': training_loss, 'valid-loss': validation_loss, 'temperature': temp,\n",
        "        'train-binary-loss': train_binary_loss, 'valid-binary-loss': valid_binary_loss,\n",
        "        'learning-rate': scheduler.get_lr()[0]\n",
        "    }\n",
        "\n",
        "    logged_values.update(training_metrics)\n",
        "    logged_values.update(validation_metrics)\n",
        "\n",
        "    train_loss.append(training_loss)\n",
        "    valid_loss.append(validation_loss)\n",
        "    train_rate.append(training_metrics['train-bit-rate'])\n",
        "    valid_rate.append(validation_metrics['valid-bit-rate'])\n",
        "    wandb.log(logged_values)\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "sisa_tensor = torch.tensor(sisa_alloc[-30_000:]).type(torch.int64).to(device)\n",
        "data_tensor = torch.tensor(cmg[-30_000:]).to(device)\n",
        "allc_tensor = model(data_tensor).to(device)\n",
        "\n",
        "print(sisa_tensor.shape, data_tensor.shape, allc_tensor.shape)\n",
        "print(update_metrics(defaultdict(lambda : 0), sisa_tensor, data_tensor, config, 6))\n",
        "print(update_metrics(defaultdict(lambda : 0), allc_tensor, data_tensor, config, 6))\n"
      ],
      "metadata": {
        "id": "Fwpez1dnp-vj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simulation Parameters: \n",
       "\n",
       "|                      name |                     value |\n",
       "---------------------------------------------------------\n",
       "|        num_of_subnetworks |                   20.0000 |\n",
       "|              n_subchannel |                    4.0000 |\n",
       "|             deploy_length |                   20.0000 |\n",
       "|             subnet_radius |                    1.0000 |\n",
       "|                      minD |                    0.8000 |\n",
       "|               minDistance |                    2.0000 |\n",
       "|                 bandwidth |             40000000.0000 |\n",
       "|              ch_bandwidth |             10000000.0000 |\n",
       "|                        fc |           6000000000.0000 |\n",
       "|                    lambdA |                    0.0500 |\n",
       "|                  clutType |                     dense |\n",
       "|                  clutSize |                    2.0000 |\n",
       "|                  clutDens |                    0.6000 |\n",
       "|                   shadStd |                    7.2000 |\n",
       "|                 max_power |                    0.0000 |\n",
       "|                    no_dbm |                 -174.0000 |\n",
       "|           noise_figure_db |                    5.0000 |\n",
       "|               noise_power |                    0.0000 |\n",
       "|       correlationDistance |                    5.0000 |\n",
       "|            transmit_power |                    0.0010 |\n",
       "---------------------------------------------------------"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple data manipulation\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# results logging\n",
    "import wandb\n",
    "\n",
    "# progress bar\n",
    "from   tqdm.notebook import tqdm, trange\n",
    "\n",
    "# remove warnings (remove deprecated warnings)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# visualization of resultsa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from   matplotlib.ticker import MaxNLocator\n",
    "import seaborn           as sns\n",
    "\n",
    "# Graph Algorithms.\n",
    "import networkx as nx\n",
    "\n",
    "# Google Colab (many lines are removed)\n",
    "import os\n",
    "import zipfile\n",
    "# from google.colab import drive\n",
    "# from distutils.dir_util import copy_tree\n",
    "\n",
    "# wheter we are using colab or not\n",
    "COLAB: bool = False\n",
    "if not COLAB and not os.path.exists('./data/simulations'): \n",
    "    os.chdir('..')\n",
    "\n",
    "# Simulation Settings\n",
    "from g6smart.sim_config import SimConfig\n",
    "from g6smart.evaluation import rate as rate_metrics\n",
    "\n",
    "config = SimConfig(0)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb(name: str, config: dict[str, float]):\n",
    "    config['name'] = name\n",
    "    wandb.init(\n",
    "        project=\"6GSmartRRM\",\n",
    "        name   = name,\n",
    "        config = config\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations and Information\n",
    "\n",
    "Thanks to the given scripts, we can load a group of generated simulations. They don't have any solutions (neither approximations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moung Google Drive Code\n",
    "if COLAB:\n",
    "    # drive.mount('/content/drive')\n",
    "\n",
    "    # Move Simulations to avoid cluttering the drive folder\n",
    "    # if not os.path.exists('/content/simulations'):\n",
    "    #   os.mkdir('/content/simulations')\n",
    "\n",
    "    # if list(os.listdir('/content/simulations')) == []:\n",
    "    #   copy_tree('/content/drive/MyDrive/TFM/simulations', '/content/simulations')\n",
    "\n",
    "    # unzip all simulations\n",
    "    # print(\"Name of the already simulated data: \\n\", )\n",
    "    for zip_file in os.listdir('/content/simulations'):\n",
    "        if zip_file.endswith('.zip'):\n",
    "            print(\" ----> \" + zip_file)\n",
    "            with zipfile.ZipFile(\"/content/simulations/\" + zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content/simulations/')\n",
    "\n",
    "    SIMULATIONS_PATH: str = \"/content/simulations\"\n",
    "else:\n",
    "    if not os.path.exists('./data/simulations'): os.mkdir('./data/simulations')\n",
    "    for zip_file in os.listdir('data'):\n",
    "        if zip_file.endswith('.zip'):\n",
    "            print(\" ----> \" + zip_file)\n",
    "            with zipfile.ZipFile(\"./data/\" + zip_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall('./data/simulations')\n",
    "    SIMULATIONS_PATH: str = \"./data/simulations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel    matrix shape: 12000 x  4 x 20 x 20 \n",
      "allocation matrix shape: 12000 x 20\n"
     ]
    }
   ],
   "source": [
    "cmg   = np.load(SIMULATIONS_PATH + '/Channel_matrix_gain.npy')\n",
    "alloc = np.load(SIMULATIONS_PATH + '/sisa-allocation.npy')\n",
    "\n",
    "# get sample from all\n",
    "n_sample = 12_000\n",
    "cmg   = cmg[:n_sample]\n",
    "alloc = alloc[:n_sample]\n",
    "\n",
    "n_sample = cmg.shape[0]\n",
    "K, N, _  = cmg.shape[1:]\n",
    "\n",
    "shape    = lambda s: \" x\".join([f\"{d:3d}\" for d in s])\n",
    "print(f\"channel    matrix shape: {shape(cmg.shape)} \\nallocation matrix shape: {shape(alloc.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publications to revise\n",
    "\n",
    "* (power) Power control for 6g industrial wireless subnetworks: A graph neural network approach\n",
    "* (allocation) Towards 6g in-x subnetworks with sub-millisecond communication cycles and extreme reliability\n",
    "* (power) Multi-agent deep reinforcement learning for dynamic power allocation in wireless networks\n",
    "* (both) Multi-agent reinforcement learning for dynamic resource management in 6g in-x subnetworks\n",
    "* (both) Multi-agent dynamic resource allocation in 6g in-x subnetworks with limited sensing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Proposal\n",
    "\n",
    "In this proposal, we could mixed different implementations for optimization of problem.\n",
    "We can only consider the following setup:\n",
    "\n",
    "1. Determine a almost optimal subband allocation for the networks. We could use a power selection of $p = p_{max}$\n",
    "2. Based on the obtained allocation, we determine a power control for each subnetwork that minimizes the used \n",
    "power and does not deteriorite the signal.\n",
    "\n",
    "For the subband allocation, we could consider the implementation from this [publication](https://ieeexplore.ieee.org/document/10597067).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Step: Subband allocation problem\n",
    "class RateConfirmAllocModel(nn.Module):\n",
    "    def __init__(self, n_subnetworks: int, n_bands: int, hidden_dim: int = 1000, hidden_layers: int = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize state\n",
    "        self.n = n_subnetworks\n",
    "        self.k = n_bands\n",
    "\n",
    "        # DNN architecture\n",
    "        self.input_size = self.n * self.n\n",
    "        self.output_size = self.n * self.k\n",
    "        \n",
    "        layers = [nn.BatchNorm1d(self.input_size)]\n",
    "        dims = [self.input_size] + [hidden_dim] * (hidden_layers - 1) + [self.output_size]\n",
    "        for i in range(1, hidden_layers + 1):\n",
    "            # linear layers with HE initialization\n",
    "            layers.append(nn.Linear(dims[i - 1], dims[i]))\n",
    "            torch.nn.init.kaiming_normal_(layers[-1].weight, nonlinearity='relu')     \n",
    "            \n",
    "            # apply dropout. We have a lot of parameters, it is required\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.05))\n",
    "            layers.append(nn.BatchNorm1d(dims[i]))\n",
    "\n",
    "        layers = layers[:-3]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(channel_gain: np.ndarray | torch.Tensor ) -> torch.Tensor:\n",
    "        if len(channel_gain.shape[1:]) == 3: \n",
    "            channel_gain = torch.mean(channel_gain, dim = 1)\n",
    "        \n",
    "        channel_gain = torch.tensor(channel_gain, requires_grad=False)\n",
    "        channel_gain = 10 * torch.log10(channel_gain) # transform to Dbm\n",
    "        return channel_gain.type(torch.float32)\n",
    "\n",
    "    def forward(self, channel_gain: torch.Tensor ) -> torch.Tensor:\n",
    "        # preprocess to obtain a NxN channel gain\n",
    "        channel_gain = self.preprocess(channel_gain)\n",
    "        # flatten the inputs\n",
    "        flattened_channel = channel_gain.reshape(-1, self.input_size)\n",
    "        # apply model\n",
    "        channel_network = self.model(flattened_channel)\n",
    "        # determine best allocation\n",
    "        channel_network = channel_network.reshape(-1, self.k, self.n)\n",
    "        # derive probabilities\n",
    "        return F.softmax(channel_network, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The used loss function in the first-stage correspond to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_rate(C: torch.Tensor, A: torch.Tensor, config: SimConfig):\n",
    "    B, K, N, _ = C.shape\n",
    "    NE         = config.noise_power\n",
    "    P          = config.transmit_power\n",
    "\n",
    "    # calculate signal and interference with probabilistic allocations\n",
    "    ids    = range(N)\n",
    "    signal = C[:, :, ids, ids] * A[:, :, :] * P\n",
    "    interference = torch.sum((C[:, :, :, :] * A[:, :, :, None]) * P, dim=2) - signal\n",
    "    interference = interference + NE + 1e-9\n",
    "    \n",
    "    # compute individual rate\n",
    "    sinr = signal / interference\n",
    "    rate = torch.log2(1 + sinr)\n",
    "    rate = torch.sum(rate, dim = 1)\n",
    "    return rate\n",
    "\n",
    "def loss_fullfield_req(config: SimConfig, channel_gain: torch.Tensor, allocation: torch.Tensor, req: float) -> torch.Tensor:\n",
    "    rate = shannon_rate(channel_gain, allocation, config)\n",
    "    rate = F.sigmoid(req - rate)\n",
    "    return torch.mean(rate / req, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ab3a90c4c1489088412658bb921543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training epoch:   0%|          | 0/2 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd1d8832188450da91ac6f1630a1826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training step::   0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb8be57794f429db882c9c9fcc529de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation step::   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train-loss': 0.07050831734227296, 'valid-loss': 0.2978861067858164, 'train-binary-loss': tensor(0.1893), 'valid-binary-loss': 0.0, 'train-bit-rate': 0.0, 'train-jain-fairness': 0.0, 'train-spectral-efficency': 0.0, 'valid-bit-rate': 0.0, 'valid-jain-fairness': 0.0, 'valid-spectral-efficency': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a12b50dfb194fbb8452033570c5c8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training step::   0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfb918b25484c7882fd5bd1ca61dc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation step::   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train-loss': 0.05494073937393671, 'valid-loss': 0.27588836982328757, 'train-binary-loss': tensor(0.2310), 'valid-binary-loss': 0.0, 'train-bit-rate': 0.0, 'train-jain-fairness': 0.0, 'train-spectral-efficency': 0.0, 'valid-bit-rate': 0.0, 'valid-jain-fairness': 0.0, 'valid-spectral-efficency': 0.0}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE: int = 1024\n",
    "MAX_EPOCH : int = 200\n",
    "\n",
    "REQ: float      = 5.\n",
    "\n",
    "# build datasets\n",
    "TRAIN_SAMPLE: int = 10_000\n",
    "VALID_SAMPLE: int =  5_000\n",
    "TESTS_SAMPLE: int =  5_000\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(cmg[:TRAIN_SAMPLE]))\n",
    "train_data = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_data = TensorDataset(torch.tensor(cmg[TRAIN_SAMPLE:TRAIN_SAMPLE + VALID_SAMPLE]))\n",
    "valid_data = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "tests_data = TensorDataset(torch.tensor(cmg[TRAIN_SAMPLE+VALID_SAMPLE:TRAIN_SAMPLE + VALID_SAMPLE + TESTS_SAMPLE]))\n",
    "tests_data = DataLoader(tests_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# training config\n",
    "LR: float  = 0.01\n",
    "\n",
    "\n",
    "name  = \"p1-alloc-dnn-00-01-base\"\n",
    "learning_config = {\n",
    "    'batch-size': BATCH_SIZE,\n",
    "    'n_epochs'  : MAX_EPOCH,\n",
    "    'lr'        : LR,\n",
    "    'dropout'   : 0.01,\n",
    "    'data-split': f\"{TRAIN_SAMPLE}-{VALID_SAMPLE}-{TESTS_SAMPLE}\",\n",
    "    'network-req-nbit': f'{REQ:3.2f}',\n",
    "    'network-req-snir': f'{2 ** (REQ) - 1}'\n",
    "}\n",
    "\n",
    "def binarization_error(alloc: torch.Tensor) -> float:\n",
    "    rounded = torch.round(alloc)\n",
    "    return torch.mean(torch.abs(alloc - rounded))\n",
    "\n",
    "def update_metrics(metrics, alloc, sample):\n",
    "    alloc = torch.argmax(alloc.detach(), dim = 1)\n",
    "\n",
    "    batch_size = sample.size(0)\n",
    "    for b in range(batch_size):\n",
    "        allocation  = alloc[b].detach().cpu().numpy()\n",
    "        gain_matrix = sample[b].detach().cpu().numpy()\n",
    "\n",
    "        rates       = rate_metrics.bit_rate(config, gain_matrix, allocation, config.transmit_power)\n",
    "        fairness    = rate_metrics.jain_fairness(rates)\n",
    "        spectral    = np.mean(rate_metrics.spectral_efficency(config, rates))\n",
    "        rates       = np.mean(rates)\n",
    "\n",
    "    metrics['bit-rate'] += rates / batch_size\n",
    "    metrics['jain-fairness'] += fairness / batch_size\n",
    "    metrics['spectral-efficency'] += spectral / batch_size\n",
    "    return metrics\n",
    "\n",
    "# setup_wandb(name, learning_config)\n",
    "model = RateConfirmAllocModel(20, 4).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "for epoch in trange(MAX_EPOCH, desc = \"training epoch\", unit = \"epoch\"):\n",
    "    # training step\n",
    "    model.train()\n",
    "    training_loss = 0.\n",
    "    train_binary_loss = 0.\n",
    "\n",
    "    training_metrics = {\n",
    "        'bit-rate': 0.,\n",
    "        'jain-fairness': 0.,\n",
    "        'spectral-efficency': 0.\n",
    "    }\n",
    "\n",
    "    for sample in tqdm(train_data, desc = 'training step:', unit = 'batch', total = len(train_data), leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample     = sample[0]\n",
    "        alloc_prob = model(sample.to(device))\n",
    "        loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_binary_loss += binarization_error(alloc_prob.detach())\n",
    "\n",
    "        # training_metrics = update_metrics(training_metrics, alloc_prob, sample)\n",
    "\n",
    "    training_loss = training_loss / len(train_data)\n",
    "    train_binary_loss = train_binary_loss / len(train_data)\n",
    "    training_metrics = { 'train-' + key: val / len(train_data) for key, val in training_metrics.items()}\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0.\n",
    "    valid_binary_loss = 0.\n",
    "    validation_metrics = {\n",
    "        'bit-rate': 0.,\n",
    "        'jain-fairness': 0.,\n",
    "        'spectral-efficency': 0.\n",
    "    }\n",
    "    for sample in tqdm(valid_data, desc = 'validation step:', unit = 'batch', total = len(valid_data), leave = False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        sample     = sample[0]\n",
    "        alloc_prob = model(sample)\n",
    "        loss       = loss_fullfield_req(config, sample, alloc_prob, REQ).mean()\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "        # validation_metrics = update_metrics(validation_metrics, alloc_prob, sample)\n",
    "\n",
    "    vaidation_loss = validation_loss / len(train_data)\n",
    "    valid_binary_loss = valid_binary_loss / len(valid_data)\n",
    "\n",
    "    validation_metrics = { 'valid-' + key: val / len(valid_data) for key, val in validation_metrics.items()}\n",
    "    \n",
    "    logged_values = {\n",
    "        'train-loss': training_loss, 'valid-loss': validation_loss, \n",
    "        'train-binary-loss': train_binary_loss, 'valid-binary-loss': valid_binary_loss\n",
    "    }\n",
    "\n",
    "    logged_values.update(training_metrics)\n",
    "    logged_values.update(validation_metrics)\n",
    "    print(logged_values)\n",
    "    # wandb.log(logged_values)\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrm-g6-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
